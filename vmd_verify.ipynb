{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Will train static y[:1000], then decompose and predict step wise simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmdpy import VMD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D, Dense, LSTM, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "# need to create validation part of testing!\n",
    "\n",
    "class vmd_lstm:\n",
    "\n",
    "    def train_data(x_train, y_train, length=21):\n",
    "        '''Train data generator. Can be many steps ahead forecast.\n",
    "        '''\n",
    "        data_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            length=length,  # t-n to t\n",
    "            sampling_rate=1,\n",
    "            stride=1,  # gap by n-days\n",
    "            start_index=0,  # ex. start on monday\n",
    "            end_index=None,\n",
    "            shuffle=False,\n",
    "            reverse=False,\n",
    "            batch_size=256\n",
    "        )  # missing target length horizon (only manual setting)\n",
    "        return data_gen\n",
    "    \n",
    "    def test_data(x_test, length=21):\n",
    "        '''Test data should not leak target,\n",
    "        y_test == np.zeros(len(x_test)).\n",
    "        '''\n",
    "        gen_day_ahead = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n",
    "            x_test,\n",
    "            np.zeros(len(x_test)),\n",
    "            length=21,  # t-n to t\n",
    "            sampling_rate=1,\n",
    "            stride=1,  # gap by n-days\n",
    "            start_index=0,  # ex. start on monday\n",
    "            end_index=None,\n",
    "            shuffle=False,\n",
    "            reverse=False,\n",
    "            batch_size=1\n",
    "        )  # missing target length horizon (only manual setting)\n",
    "\n",
    "        return gen_day_ahead\n",
    "    \n",
    "    def lstm(input_data, epochs=500, learning_rate=1e-3):\n",
    "        '''Custom model LSTM-DNN.\n",
    "        Returns keras.models.Sequential()\n",
    "        formula and prints log(loss) plot for reference.\n",
    "        '''\n",
    "        model = keras.models.Sequential()\n",
    "        # model.add(keras.layers.Input(shape=(21, 13)))\n",
    "        # Can have CNN layer if we have many IMFs (test)\n",
    "        model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "        model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss=['mse'])\n",
    "        a = model.fit(input_data, shuffle=False, epochs=epochs, workers=-1, use_multiprocessing=True)\n",
    "        plt.figure(figsize=(14,8))\n",
    "        plt.plot(np.log1p(a.history['loss']))\n",
    "        plt.title(\"log(Loss history)\")\n",
    "        plt.grid()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def vmd(input_data, n_modes=13, alpha=200, tol=1e-20, plot=True):\n",
    "        '''Input data and decompose it to n-modes.\n",
    "        Function to be used in sensitivity analysis in pre-processing part.\n",
    "        Returns decomposed_data , y_hat, omega.\n",
    "        Use plot=False when grid searching for parameters.\n",
    "        '''\n",
    "        # Params  \n",
    "        alpha = alpha  # quadratic penalty, high alpha == bias (sensitivity analysis)\n",
    "        tau = 0.  # noise-tolerance (no strict fidelity enforcement)  \n",
    "        K = n_modes  # modes count (sensitivity analysis)\n",
    "        DC = 0  # no DC part imposed  \n",
    "        init = 1  # initialize omegas uniformly  \n",
    "        tol = tol  \n",
    "\n",
    "        # VMD \n",
    "        modes, u_hat, omega = VMD(input_data, alpha, tau, K, DC, init, tol)  \n",
    "\n",
    "        # Reconstruct data\n",
    "        a = input_data\n",
    "        reconstruct = np.sum(pd.DataFrame(modes))  # additive components; shape==(503, 13)\n",
    "        rmse = np.sqrt(1/len(a) * np.sum(np.square(a-reconstruct)))\n",
    "        mape = np.mean(np.abs(reconstruct/a-1))\n",
    "    \n",
    "        if plot == True:\n",
    "            # Visualize decomposed modes\n",
    "            plt.figure(figsize=(14,8))\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.plot(input_data)\n",
    "            plt.title('Original signal')\n",
    "            plt.subplot(2,1,2)\n",
    "            plt.plot(modes.T)\n",
    "            plt.title('Decomposed modes')\n",
    "            plt.legend(['Mode %d'%m_i for m_i in range(modes.shape[0])])\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.figure(figsize=(14,8))\n",
    "            plt.plot(y-reconstruct)\n",
    "            plt.title(\"Reconstruction error\")\n",
    "            plt.figure(figsize=(14,8))\n",
    "            plt.hist(y-reconstruct, bins='fd')\n",
    "\n",
    "        return modes.T, rmse, mape\n",
    "    \n",
    "    def predict_test(x_test, len=21):\n",
    "        '''x_test = Test data generated \n",
    "        from tf.TimeseriesGenerator()\n",
    "        '''\n",
    "        offset_length = 100 - len\n",
    "        y_act = pd.Series(y[-offset_length:])  # uses global y\n",
    "        y_pred = model.predict(x_test)  # weird format\n",
    "        y_pred = pd.Series(y_pred.T[0])\n",
    "        plt.figure(figsize=(32,8))\n",
    "        plt.plot(y_act, label='actual')\n",
    "        plt.plot(y_pred, c='r', label='pred')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        return y_pred, y_act\n",
    "\n",
    "    def predict_train(x_train, len=21):\n",
    "        '''x_train = Train data generated \n",
    "        from tf.TimeseriesGenerator()\n",
    "        '''\n",
    "        offset_length = 1000 - len\n",
    "        y_act = pd.Series(y[:1000]).shift(-21)\n",
    "        y_pred = model.predict(data_gen)\n",
    "        plt.figure(figsize=(32,8))\n",
    "        plt.plot(y_act, label='actual')\n",
    "        plt.plot(y_pred, c='r', label='pred')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        \n",
    "        return y_pred, y_act"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclution: If we dont use pipeline with pre-processing -> modeling -> testing at each step, we are doing VMD wrong!\n",
    "\n",
    "**Doing all dataset to IMFs and then slicing for test set is leaking the target into test set!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo and organize:\n",
    "- Pre-process/model/test unseen per steps and collect in for loop\n",
    "- Organize all into one big for loop that will collect steps of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "### Pipeline simulation\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "# Download and prepare financial data\n",
    "spy_df = yf.download(tickers = \"SPY\",  # list of tickers\n",
    "                period = \"5y\",         # time period\n",
    "                interval = \"1d\",       # trading interval\n",
    "                prepost = False,       # download pre/post market hours data?\n",
    "                repair = True)\n",
    "spy_pct = np.log1p(spy_df[\"Adj Close\"].pct_change().dropna())\n",
    "spy_pct = np.array(spy_pct)\n",
    "\n",
    "y_total = spy_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "CPU times: total: 4min 51s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Simulator\n",
    "collect = []\n",
    "coef = list(range(1000, len(spy_pct), 2))\n",
    "i = 0\n",
    "for i in coef:\n",
    "    # Prepare slice for vmd\n",
    "    y = spy_pct[:i]\n",
    "\n",
    "    # IMFs input data (vmd param search)\n",
    "    modes, _, _ = vmd_lstm.vmd(y, n_modes=29, alpha=50, plot=False)  # n_modes=K, alpha=alpha\n",
    "\n",
    "    x_train, y_train = modes, y\n",
    "    x_test = np.concatenate([modes[-21:], [np.zeros(29)]])\n",
    "    data_gen = vmd_lstm.train_data(x_train, y_train)\n",
    "    data_gen_dayahead = vmd_lstm.test_data(x_test)  # take y_total outside of the loop for comparison\n",
    "    \n",
    "    # Train (only initially)\n",
    "    # if i == 1000:\n",
    "    #     model = vmd_lstm.lstm(data_gen, epochs=800, learning_rate=1e-4)\n",
    "\n",
    "    # Pred seen data (optional)\n",
    "    # y_pred, y_act = vmd_lstm.predict_train(data_gen)\n",
    "\n",
    "    # Pred unseen day ahead\n",
    "    step_ahead = model.predict(data_gen_dayahead)  # this will pred t+1 ex. y[1001] on first iter\n",
    "    collect.append([i, step_ahead])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_df = pd.DataFrame(collect)\n",
    "collect_df.to_csv(\"simulator_collect.csv\")  # currently every second day in order to meet VMD criteria of even decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 29)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1001, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem (decomposing odd amount of rows leads to one less row in modes dataset)\n",
    "y_total = y_total[:1001]\n",
    "modes = vmd_lstm.vmd(y_total, n_modes=29, alpha=50, plot=False)\n",
    "display(pd.DataFrame(modes).shape)\n",
    "display(pd.DataFrame(y_total).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Potential fix with odd number problem for decomposition can be shifting between first slice index y[0/1:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
